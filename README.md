# Лабораторная работа №5
Выполнил: Шайдуров Даниил Сергеевич

Цель работы: получить навыки сегментации изображений дорожного движения
с помощью модели нейронной сети U-Net.

Задачи:
1. Ознакомиться с работой сверточных нейронных сетей в библиотеке Keras;
2. Научиться решать задачу сегментации изображений на основе нейронных
сетей.
3. Научиться применять модель сверточной нейронной сети U-Net.

## 1. Определение значений loss-функции, при которых маски сегментации выглядят правдоподобно

Базовая модель u-net без бэкбона обучалась на 60 эпохах. Удовлетворительного результата так и не получилось достигнуть 
по причине простой архитектуру нейросети и небольшого объема датасета. Минимальное значение лосс-функции на валидации 1,4. 
Ниже представлены скрины на 14 и 60 эпохе обучения. 

Эпоха 14
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unet/epoch_14_comparison.png)

Эпоха 60 
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unet/epoch_59_comparison.png)

Значения лосс-функции на тренировочных и валидационных данных. 
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unet/unet.png)


U-net спроектированная на базе предобученной модели vgg показала результата лучше. Уже к 7-й эпохе обучения модель 
продемонстрировала результаты, сопоставимые с показателями базовой U-Net после 60 эпох. 
Минимальное значение лосс-функции на валидации 0.7

Эпоха 7
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unetvgg/epoch_14_comparison.png)

Эпоха 60
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unetvgg/epoch_59_comparison.png)

Значения лосс-функции на тренировочных и валидационных данных. 
![images](https://github.com/DaniilShd/NN_lab_5/blob/main/result_image/unetvgg/unetvgg.png)

Правдоподобные маски сегментации начинают получаться при значениях loss-функции (Binary Cross-Entropy) в диапазоне [0.5–0.7].

Вывод:

При loss > 1.0 маски часто содержат шум или фрагментированные области.
При loss < 0.5 модель достигает высокой точности.
Оптимальный баланс между скоростью обучения и качеством достигается при loss ~ 0.5.

## 2. Сравнение количества эпох для U-Net "с нуля" и U-Net на основе VGG

Предобученный бэкбон (VGG) ускоряет обучение в 3–5 раза за счет использования уже оптимизированных весов для feature extraction.
Качество сегментации на ранних этапах (первые 10 эпох) у VGG-based U-Net значительно выше, чем у модели, обученной с нуля.
Рекомендация: Использовать предобученные бэкбоны при ограниченных вычислительных ресурсах или малом размере датасета.

## Итоговые выводы
Loss-функция ниже 0.5 — индикатор хорошего качества масок.
Предобученные бэкбоны (VGG) ускоряют обучение и улучшают качество, особенно на малых датасетах.
Визуальная оценка подтверждает, что U-Net с переносом обучения требует меньше эпох для сопоставимого или лучшего результата.